{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "093a3c41-a14c-45b8-a0d1-baa3ad72052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from densenet import densenet169\n",
    "from utils import plot_training, n_p, get_count\n",
    "from train import train_model, get_metrics\n",
    "from pipeline import get_study_level_data, get_dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d29e023d-9c62-43d9-a7ac-8d70806f9718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3267/3267 [00:39<00:00, 83.38it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 207/207 [00:01<00:00, 128.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# #### load study level dict data\n",
    "study_data = get_study_level_data(study_type='XR_WRIST')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02040156-bd6a-451f-89ee-75d68fb48f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Create dataloaders pipeline\n",
    "data_cat = ['train', 'valid'] # data categories\n",
    "dataloaders = get_dataloaders(study_data, batch_size=1)\n",
    "dataset_sizes = {x: len(study_data[x]) for x in data_cat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8ef2a9d-f681-4aff-9782-df5631391e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Build model\n",
    "# tai = total abnormal images, tni = total normal images\n",
    "tai = {x: get_count(study_data[x], 'positive') for x in data_cat}\n",
    "tni = {x: get_count(study_data[x], 'negative') for x in data_cat}\n",
    "Wt1 = {x: n_p(tni[x] / (tni[x] + tai[x])) for x in data_cat}\n",
    "Wt0 = {x: n_p(tai[x] / (tni[x] + tai[x])) for x in data_cat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "735e1e9f-6d17-475f-8662-fe4cefe5a94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tai: {'train': 3987, 'valid': 295}\n",
      "tni: {'train': 5769, 'valid': 364} \n",
      "\n",
      "Wt0 train: tensor([0.4087])\n",
      "Wt0 valid: tensor([0.4476])\n",
      "Wt1 train: tensor([0.5913])\n",
      "Wt1 valid: tensor([0.5524])\n"
     ]
    }
   ],
   "source": [
    "print('tai:', tai)\n",
    "print('tni:', tni, '\\n')\n",
    "print('Wt0 train:', Wt0['train'])\n",
    "print('Wt0 valid:', Wt0['valid'])\n",
    "print('Wt1 train:', Wt1['train'])\n",
    "print('Wt1 valid:', Wt1['valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cc5f2f7-e5a8-49c7-98fb-89d2a30e1984",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(torch.nn.modules.Module):\n",
    "    def __init__(self, Wt1, Wt0):\n",
    "        super(Loss, self).__init__()\n",
    "        self.Wt1 = Wt1\n",
    "        self.Wt0 = Wt0\n",
    "        \n",
    "    def forward(self, inputs, targets, phase):\n",
    "        loss = - (self.Wt1[phase] * targets * inputs.log() + self.Wt0[phase] * (1 - targets) * (1 - inputs).log())\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef6e6830-b811-48d7-a148-070f79ebb46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paula\\Documents\\Repositorio\\Test\\densenet.py:115: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  nn.init.kaiming_normal(m.weight.data)\n"
     ]
    }
   ],
   "source": [
    "model = densenet169(pretrained=True)\n",
    "#model = model.cuda()\n",
    "\n",
    "#model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet169', pretrained=True)\n",
    "#model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f052da04-424d-436d-8039-c0b4a7b5da64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paula\\miniforge3\\envs\\tfg\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "criterion = Loss(Wt1, Wt0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99878fa9-cd23-40ba-aa9f-a5bb1e315b6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 3460, 'valid': 237}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c15fd12-ca15-48ab-a0e7-47dfbb4a869e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('train', <torch.utils.data.dataloader.DataLoader object at 0x000002D8B94424B0>), ('valid', <torch.utils.data.dataloader.DataLoader object at 0x000002D8A92096A0>)])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloaders.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9482d5a6-8613-45ca-b142-c1a7a3186bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 3460\n",
      "Valid batches: 237 \n",
      "\n",
      "Epoch 1/5\n",
      "----------\n",
      "0\r"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# #### Train model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\Repositorio\\Test\\train.py:53\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, criterion, optimizer, dataloaders, scheduler, dataset_sizes, num_epochs)\u001b[0m\n\u001b[0;32m     51\u001b[0m     labels\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mFloatTensor)\n\u001b[0;32m     52\u001b[0m     running_corrects \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(preds \u001b[38;5;241m==\u001b[39m labels\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m---> 53\u001b[0m     \u001b[43mconfusion_matrix\u001b[49m\u001b[43m[\u001b[49m\u001b[43mphase\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;66;03m#confusion_matrix[phase].add(preds.squeeze(), labels.data.squeeze())\u001b[39;00m\n\u001b[0;32m     55\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m dataset_sizes[phase]\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\tfg\\Lib\\site-packages\\torchnet\\meter\\confusionmeter.py:44\u001b[0m, in \u001b[0;36mConfusionMeter.add\u001b[1;34m(self, predicted, target)\u001b[0m\n\u001b[0;32m     41\u001b[0m predicted \u001b[38;5;241m=\u001b[39m predicted\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     42\u001b[0m target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m---> 44\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[43mpredicted\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m target\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \\\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumber of targets and predicted outputs do not match\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(predicted) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m predicted\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk, \\\n\u001b[0;32m     49\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumber of predictions does not match size of confusion matrix\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# #### Train model\n",
    "model = train_model(model, criterion, optimizer, dataloaders, scheduler, dataset_sizes, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4272bac-2426-49b8-823d-ba93ad185996",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fa6a38-0161-4696-be58-a629ff476536",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics(model, criterion, dataloaders, dataset_sizes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
