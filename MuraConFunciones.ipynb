{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "############ CARGAMOS ESTUDIO \n",
    "###########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR   = 'C:/Users/Usuario/Datasets/MURA-v1.1/' # directorio raíz\n",
    "study_type = 'XR_WRIST' # tipo de estudio\n",
    "\n",
    "DATA_CAT = ['train', 'valid'] # división de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_study_level_data(study_type):\n",
    "    \"\"\"\n",
    "    Returns a dict, with keys 'train' and 'valid' and respective values as study level dataframes, \n",
    "    these dataframes contain three columns 'Path', 'Count', 'Label'\n",
    "    Args:\n",
    "        study_type (string): one of the seven study type folder names in 'train/valid/test' dataset \n",
    "    \"\"\"\n",
    "    study_data = {}\n",
    "    study_label = {'positive': 1, 'negative': 0}\n",
    "    for phase in DATA_CAT:\n",
    "        BASE_DIR = ROOT_DIR + '%s/%s/' % (phase, study_type)\n",
    "        print(BASE_DIR, '\\n')\n",
    "        #patients = list(os.walk(BASE_DIR))[0][1] # list of patient folder names\n",
    "        patients = os.listdir(BASE_DIR)\n",
    "        study_data[phase] = pd.DataFrame(columns=['Path', 'Count', 'Label'])\n",
    "        i = 0\n",
    "        for patient in tqdm(patients): # for each patient folder\n",
    "            for study in os.listdir(BASE_DIR + patient): # for each study in that patient folder\n",
    "                label = study_label[study.split('_')[1]] # get label 0 or 1\n",
    "                path = BASE_DIR + patient + '/' + study + '/' # path to this study\n",
    "                study_data[phase].loc[i] = [path, len(os.listdir(path)), label] # add new row\n",
    "                i+=1\n",
    "    return study_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/Usuario/Datasets/MURA-v1.1/train/XR_WRIST/ \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3267 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3267/3267 [00:07<00:00, 462.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/Usuario/Datasets/MURA-v1.1/valid/XR_WRIST/ \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [00:00<00:00, 479.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# #### load study level dict data\n",
    "study_data = get_study_level_data(study_type='XR_WRIST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>Count</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:/Users/Usuario/Datasets/MURA-v1.1/train/XR_W...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:/Users/Usuario/Datasets/MURA-v1.1/train/XR_W...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:/Users/Usuario/Datasets/MURA-v1.1/train/XR_W...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:/Users/Usuario/Datasets/MURA-v1.1/train/XR_W...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:/Users/Usuario/Datasets/MURA-v1.1/train/XR_W...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Path  Count  Label\n",
       "0  C:/Users/Usuario/Datasets/MURA-v1.1/train/XR_W...      3      1\n",
       "1  C:/Users/Usuario/Datasets/MURA-v1.1/train/XR_W...      4      0\n",
       "2  C:/Users/Usuario/Datasets/MURA-v1.1/train/XR_W...      3      1\n",
       "3  C:/Users/Usuario/Datasets/MURA-v1.1/train/XR_W...      3      1\n",
       "4  C:/Users/Usuario/Datasets/MURA-v1.1/train/XR_W...      2      0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study_data['train'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>Count</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:/Users/Usuario/Datasets/MURA-v1.1/valid/XR_W...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:/Users/Usuario/Datasets/MURA-v1.1/valid/XR_W...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:/Users/Usuario/Datasets/MURA-v1.1/valid/XR_W...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:/Users/Usuario/Datasets/MURA-v1.1/valid/XR_W...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:/Users/Usuario/Datasets/MURA-v1.1/valid/XR_W...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Path  Count  Label\n",
       "0  C:/Users/Usuario/Datasets/MURA-v1.1/valid/XR_W...      4      1\n",
       "1  C:/Users/Usuario/Datasets/MURA-v1.1/valid/XR_W...      2      1\n",
       "2  C:/Users/Usuario/Datasets/MURA-v1.1/valid/XR_W...      3      1\n",
       "3  C:/Users/Usuario/Datasets/MURA-v1.1/valid/XR_W...      3      1\n",
       "4  C:/Users/Usuario/Datasets/MURA-v1.1/valid/XR_W...      1      1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study_data['valid'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "######## CREAMOS DATA LOADER Y CLASE DATASET ASOCIADA\n",
    "###########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    \"\"\"training dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, df, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (pd.DataFrame): a pandas DataFrame with image path and labels.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        study_path = self.df.iloc[idx, 0]\n",
    "        count = self.df.iloc[idx, 1]\n",
    "        images = []\n",
    "        for i in range(count):\n",
    "            image = pil_loader(study_path + 'image%s.png' % (i+1))\n",
    "            images.append(self.transform(image))\n",
    "        images = torch.stack(images)\n",
    "        label = self.df.iloc[idx, 2]\n",
    "        sample = {'images': images, 'label': label}\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(data, batch_size=8, study_level=False):\n",
    "    '''\n",
    "    Returns dataloader pipeline with data augmentation\n",
    "    '''\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomRotation(10),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n",
    "        ]),\n",
    "        'valid': transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "    image_datasets = {x: ImageDataset(data[x], transform=data_transforms[x]) \n",
    "                                      for x in DATA_CAT}\n",
    "    dataloaders = {x: DataLoader(image_datasets[x], batch_size=batch_size, \n",
    "                                 shuffle=True, num_workers=4) \n",
    "                                 for x in DATA_CAT}\n",
    "    \n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = get_dataloaders(study_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x000002A8C6BABA70>\n"
     ]
    }
   ],
   "source": [
    "print(dataloaders['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x000002A8C6BAAB10>\n"
     ]
    }
   ],
   "source": [
    "print(dataloaders['valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sizes = {x: len(study_data[x]) for x in DATA_CAT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 3460, 'valid': 237}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "############### SE CONSTRUYE EL MODELO\n",
    "####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(df, cat):\n",
    "    '''\n",
    "    Returns number of images in a study type dataframe which are of abnormal or normal\n",
    "    Args:\n",
    "    df -- dataframe\n",
    "    cat -- category, \"positive\" for abnormal and \"negative\" for normal\n",
    "    '''\n",
    "    return df[df['Path'].str.contains(cat)]['Count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tai = total abnormal images, tni = total normal images\n",
    "tai = {x: get_count(study_data[x], 'positive') for x in DATA_CAT}\n",
    "tni = {x: get_count(study_data[x], 'negative') for x in DATA_CAT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tai: {'train': 3987, 'valid': 295}\n",
      "tni: {'train': 5769, 'valid': 364} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('tai:', tai)\n",
    "print('tni:', tni, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_p(x):\n",
    "    '''convert numpy float to Variable tensor float'''    \n",
    "    return Variable(torch.cuda.FloatTensor([x]), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_29524\\1121819801.py:3: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\tensor\\python_tensor.cpp:80.)\n",
      "  return Variable(torch.cuda.FloatTensor([x]), requires_grad=False)\n"
     ]
    }
   ],
   "source": [
    "Wt1 = {x: n_p(tni[x] / (tni[x] + tai[x])) for x in DATA_CAT}\n",
    "Wt0 = {x: n_p(tai[x] / (tni[x] + tai[x])) for x in DATA_CAT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wt0 train: tensor([0.4087], device='cuda:0')\n",
      "Wt0 valid: tensor([0.4476], device='cuda:0')\n",
      "Wt1 train: tensor([0.5913], device='cuda:0')\n",
      "Wt1 valid: tensor([0.5524], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print('Wt0 train:', Wt0['train'])\n",
    "print('Wt0 valid:', Wt0['valid'])\n",
    "print('Wt1 train:', Wt1['train'])\n",
    "print('Wt1 valid:', Wt1['valid'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################\n",
    "############### ENTRENAMIENTO DEL MODELO\n",
    "#############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de pérdida \n",
    "class Loss(torch.nn.modules.Module):\n",
    "    def __init__(self, Wt1, Wt0):\n",
    "        super(Loss, self).__init__()\n",
    "        self.Wt1 = Wt1\n",
    "        self.Wt0 = Wt0\n",
    "        \n",
    "    def forward(self, inputs, targets, phase):\n",
    "        loss = - (self.Wt1[phase] * targets * inputs.log() + self.Wt0[phase] * (1 - targets) * (1 - inputs).log())\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importación desde el fichero densenet.py del repositorio\n",
    "from densenet import densenet169"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\SW Repositories\\TFTs\\DenseNet-MURA-PyTorch\\densenet.py:115: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  nn.init.kaiming_normal(m.weight.data)\n",
      "Downloading: \"https://download.pytorch.org/models/densenet169-b2777c0a.pth\" to C:\\Users\\Usuario/.cache\\torch\\hub\\checkpoints\\densenet169-b2777c0a.pth\n",
      "100%|██████████| 54.7M/54.7M [00:08<00:00, 6.93MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = densenet169(pretrained=True)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\miniconda3\\envs\\tfgpaula\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "criterion = Loss(Wt1, Wt0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(costs, accs):\n",
    "    '''\n",
    "    Plots curve of Cost vs epochs and Accuracy vs epochs for 'train' and 'valid' sets during training\n",
    "    '''\n",
    "    train_acc = accs['train']\n",
    "    valid_acc = accs['valid']\n",
    "    train_cost = costs['train']\n",
    "    valid_cost = costs['valid']\n",
    "    epochs = range(len(train_acc))\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1,)\n",
    "    plt.plot(epochs, train_acc)\n",
    "    plt.plot(epochs, valid_acc)\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.title('Accuracy')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_cost)\n",
    "    plt.plot(epochs, valid_cost)\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.title('Cost')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, dataloaders, scheduler, \n",
    "                dataset_sizes, num_epochs):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    costs = {x:[] for x in DATA_CAT} # for storing costs per epoch\n",
    "    accs = {x:[] for x in DATA_CAT} # for storing accuracies per epoch\n",
    "    print('Train batches:', len(dataloaders['train']))\n",
    "    print('Valid batches:', len(dataloaders['valid']), '\\n')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        #confusion_matrix = {x: meter.ConfusionMeter(2, normalized=True) \n",
    "        #                    for x in DATA_CAT}\n",
    "        \n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-' * 10)\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in DATA_CAT:\n",
    "            model.train(phase=='train')\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "           \n",
    "            # Iterate over data.\n",
    "            for i, data in enumerate(dataloaders[phase]):\n",
    "                # get the inputs\n",
    "                print(i, end='\\r')\n",
    "                inputs = data['images'][0]\n",
    "                labels = data['label'].type(torch.FloatTensor)\n",
    "                # wrap them in Variable\n",
    "                inputs = Variable(inputs.cuda())\n",
    "                labels = Variable(labels.cuda())\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # forward\n",
    "                outputs = model(inputs)\n",
    "                outputs = torch.mean(outputs)\n",
    "                loss = criterion(outputs, labels, phase)\n",
    "                running_loss += loss.data[0]\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                # statistics\n",
    "                preds = (outputs.data > 0.5).type(torch.cuda.FloatTensor)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                #confusion_matrix[phase].add(preds, labels.data)\n",
    "            \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "            costs[phase].append(epoch_loss)\n",
    "            accs[phase].append(epoch_acc)\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            #print('Confusion Meter:\\n', confusion_matrix[phase].value())\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'valid':\n",
    "                scheduler.step(epoch_loss)\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "        time_elapsed = time.time() - since\n",
    "        print('Time elapsed: {:.0f}m {:.0f}s'.format(\n",
    "                time_elapsed // 60, time_elapsed % 60))\n",
    "        print()\n",
    "    \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best valid Acc: {:4f}'.format(best_acc))\n",
    "    plot_training(costs, accs)\n",
    "    \n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 3460\n",
      "Valid batches: 237 \n",
      "\n",
      "Epoch 1/5\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, criterion, optimizer, dataloaders, scheduler, dataset_sizes, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################\n",
    "############### MÉTRICAS\n",
    "#############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(model, criterion, dataloaders, dataset_sizes, phase='valid'):\n",
    "    '''\n",
    "    Loops over phase (train or valid) set to determine acc, loss and \n",
    "    confusion meter of the model.\n",
    "    '''\n",
    "    #confusion_matrix = meter.ConfusionMeter(2, normalized=True)\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    for i, data in enumerate(dataloaders[phase]):\n",
    "        print(i, end='\\r')\n",
    "        labels = data['label'].type(torch.FloatTensor)\n",
    "        inputs = data['images'][0]\n",
    "        # wrap them in Variable\n",
    "        inputs = Variable(inputs.cuda())\n",
    "        labels = Variable(labels.cuda())\n",
    "        # forward\n",
    "        outputs = model(inputs)\n",
    "        outputs = torch.mean(outputs)\n",
    "        loss = criterion(outputs, labels, phase)\n",
    "        # statistics\n",
    "        running_loss += loss.data[0] * inputs.size(0)\n",
    "        preds = (outputs.data > 0.5).type(torch.cuda.FloatTensor)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        #confusion_matrix.add(preds, labels.data)\n",
    "\n",
    "    loss = running_loss / dataset_sizes[phase]\n",
    "    acc = running_corrects / dataset_sizes[phase]\n",
    "    print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, loss, acc))\n",
    "    #print('Confusion Meter:\\n', confusion_matrix.value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics(model, criterion, dataloaders, dataset_sizes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
